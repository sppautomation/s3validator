= S3 Object Storage Provider Validation
for IBM Spectrum Protect Plus
// For displaying images in GitHub, we need to specify the absolute URL
// for the images directory. For everything else, we specify a relative path.
// ifdef::env-github[]
// :imagesdir: https://raw.githubusercontent.com/SidBB/s3validator/sidbb_doc/doc/images
// endif::[]
// ifndef::env-github[]
// :imagesdir: ./images
// endif::[]
:doctype: book
:toc: left
:toclevels: 3
:icons: font
:pagenums:
:sectnums:
:pdf-page-size: letter
:source-highlighter: highlight.js

NOTE: Copyright 2019 IBM Corp. All rights reserved. The contents of this document and any attachments are strictly confidential and are supplied on the understanding that they will be held confidentially and not disclosed to third parties without the prior written consent of IBM.

WARNING: Draft document. This is a work-in-progress.

<<<

== Introduction

This document describes the automated functional and performance tests that you can
run to validate that a cloud object storage provider is compatible with IBM Spectrum Protect Plus.

Running these tests will verify that IBM Spectrum Protect Plus can successfully offload data to and restore from an S3-compatible storage provider. The tests will also allow you to measure performance and analyze how well the solution scales.

<<<

== Basics

IBM Spectrum Protect Plus (SPP) works by writing data to vSnap servers which serve as the primary backup destination. Data in vSnap servers can then be offloaded/copied to S3-compatible object storage providers using an incremental-forever approach.

A typical deployment usually consists of one SPP server which acts as the control plane, plus one or more vSnap servers which store data.

SPP is distributed as a pre-built virtual appliance that can be deployed in VMware or Hyper-V environments. vSnap can be similarly deployed as a pre-built virtual appliance, or it can be installed on a custom physical or virtual Linux system that meets certain pre-requisites.

In order to run the validation tests, you do not need to set up an SPP server, but you must at least install and configure a vSnap server, as described in the `Prerequisites` section.

The validation tests are distributed as a `tar.gz` archive consisting of a Python-based test suite and associated configuration files. The tests are designed to run locally on a vSnap server. They work by generating a random set of data, writing it to the vSnap server's disk-based storage pool, and then offloading/copying the data to the object storage. The tests also restore data by downloading it from object storage and verifying that its contents are valid.

<<<

== Prerequisites

Set up a vSnap server by deploying a virtual appliance or running the installer on a custom Linux system. For simplicity and ease of deployment, using the virtual appliance is highly recommended.

Detailed instructions for installing, configuring, and managing a vSnap server can be found https://www.ibm.com/support/knowledgecenter/en/SSNQFQ_10.1.5/spp/t_spp_install_vsnap.html[here].

The key steps required for configuring the vSnap server for the purpose of the validation tests are summarized below.

=== Deploy the vSnap virtual appliance

* Deploy the vSnap OVA in a VMware environment using the `Deploy OVF Template` option.
* Enter network properties for the virtual machine as part of the deployment wizard, or leave them blank to use DHCP. You can also configure the network properties at a later time once the virtual machine is up and running.
* Once deployment completes, before powering on the virtual machine, edit its settings to adjust the virtual hardware. The following values are recommended for a system intended for the validation tests:
** Number of vCPUs: 8
** Total Memory: 40 GB
** Number of network adapters: 1
* Power on the virtual machine and login using the default credentials:
** Username: `serveradmin`
** Password: `sppDP758-SysXyz`
* Upon initial login, you are prompted to change the default password. Enter the default password first, then enter the new password twice. You may be forced to log out and log back in after changing the password.

NOTE: The `serveradmin` user has `sudo` privileges.

=== Configure network settings

If you did not previously configure the network properties, you can do so by running `sudo nmtui` while logged in as the `serveradmin` user.

NOTE: Using a 10 Gbit network adapter is recommended.

=== Configure storage

For the purpose of running the validation tests you must configure two storage areas:

* The primary disk-based storage pool where data is initially written.
* A disk-based cache area where data is temporarily staged while it is being offloaded/copied from the primary storage pool to the object storage.

*Primary storage pool*

The primary storage pool is the data repository where backup data is written. On a freshly installation of vSnap, no storage pool exists. As part of the vSnap initialization process, the pool is created using one or more SCSI disks.

By default, the vSnap virtual appliance includes an unused 100 GB SCSI disk which can be used to create the storage pool. If you plan to run basic tests to verify functionality and/or measure performance using a data set that is smaller than *100 GB*, it is sufficient to use the default disk to create the storage pool. If you plan to test with larger data sets, you may want to attach additional disks to create a larger storage pool.

Run `vsnap disk show` to list disks and confirm that one or more unused SCSI disks are available. The sample output below shows one unused 100 GB disk (`/dev/sdb`):

----
[serveradmin@vsnap ~]$ vsnap disk show
UUID                             | TYPE | VENDOR | MODEL        | SIZE     | USED AS     | KNAME | NAME
-----------------------------------------------------------------------------------------------------------
6000c29c116da8f495b2039fcd7fa3c3 | SCSI | VMware | Virtual disk | 70.00GB  | LVM2_member | sda   | /dev/sda
6000c293f48c897ded5c3b50afb7ca28 | SCSI | VMware | Virtual disk | 100.00GB | unused      | sdb   | /dev/sdb
6000c294c22b7968054789932dcf6621 | SCSI | VMware | Virtual disk | 128.00GB | LVM2_member | sdc   | /dev/sdc
----

To use a storage pool larger than 100 GB, attach one or more additional disks to the system, run `vsnap disk rescan` and then rerun `vsnap disk show` to confirm that they are all recognized as being unused.

To initialize the vSnap system, run `vsnap system init`. As part of the initialization process, vSnap creates a storage pool using all available unused disks

Afterwards, run `vsnap pool show` to confirm that a storage pool has been created.

Sample output:

----
[serveradmin@vsnap ~]$ vsnap pool show
TOTAL: 1

ID: 1
NAME: primary
POOL TYPE: raid0
STATUS: ONLINE
HEALTH: 100
COMPRESSION: Yes
COMPRESSION RATIO: 1.00
DEDUPLICATION: No
DEDUPLICATION RATIO: 1.00
ENCRYPTION:
    ENABLED: No

TOTAL SPACE: 99.99GB
FREE SPACE: 96.39GB
USED SPACE: 3.60GB
DATA SIZE BEFORE DEDUPLICATION: 134.50KB
DATA SIZE BEFORE COMPRESSION: 53.50KB
CREATED: 2020-01-06 20:19:33 UTC
UPDATED: 2020-01-06 20:19:33 UTC
DISKS PER RAID GROUP: 1
DISKS IN POOL:
    RAID0:
        /dev/sdb1
----

*Cache area*

By default, the vSnap virtual appliance includes a 128 GB XFS filesystem mounted at `/opt/vsnap-data` which is used as the cache area. If you plan to run basic tests to verify functionality and/or measure performance using a storage pool that is smaller than *10 TB*, it is sufficient to use the default 128 GB cache area.

If you plan to test with larger data sets, you may want to attach one or more additional disks and expand the `/opt/vsnap-data` filesystem.

To expand the cache area, attach one or more disks to the system, run `vsnap disk rescan` and then rerun `vsnap disk show` to confirm that they are all recognized as being unused.

The `/opt/vsnap-data` filesystem sits on an LVM logical volume named `vsnapdatalv` within a volume group named `vsnapdata`.  Use the following commands to create a physical volume, add it to the existing volume group, expand the logical volume, and then extend the XFS filesystem.

The sample commands below assume that a new unused disk named `/dev/sdx` has been added.

----
sudo pvcreate /dev/sdx

sudo vgextend vsnapdata /dev/sdx

sudo lvextend -l 100%VG /dev/mapper/vsnapdata-vsnapdatalv

sudo xfs_growfs /dev/mapper/vsnapdata-vsnapdatalv
----

Finally, run `df -h` and verify that the volume `/opt/vsnap-data` is mounted and has the desired new size.

<<<

== Installation

=== Download and install the test suite

*TODO*: Update this section before release to recommend using `wget <url>` pointing to a stable release instead of `git clone <url>`.

Run the following commands as the `serveradmin` user. This will install `git`, install the most up-to-date SSL certificates, and then clone the Git repository containing the test suite.

----
sudo yum --enablerepo=base,updates install git
sudo yum --enablerepo=base,updates reinstall ca-certificates
cd ~
git clone https://github.com/sppautomation/s3validator.git
----

The repository is downloaded to the directory `s3validator` under your home directory.

NOTE: If a previous version of the directory `s3validator` already exists, remove it first using `rm -rf s3validator` before using the `git clone` command above.

Then, invoke the installation script:

----
cd ~/s3validator
./install.sh
----

Sample output:

----
Creating virtual environment under: /home/serveradmin/s3validator_venv
Installing dependencies

[Output truncated]

Installation complete
----

The installation script creates a Python virtual environment in `s3validator_venv` under your home directory, then downloads and installs some dependencies in the virtual environment. If an existing `s3validator_env` directory is found, the installer removes it and creates a new one.

Once the installation is complete, you are ready to configure and run the validation tests.

<<<

== Usage

=== Overview

The test suite consists of the following categories of tests. The next few sections of this document describe the detailed configuration for driving these tests.

*Functional test*

This test evaluates the basic functionality of the offload feature.

The test uploads data to the S3 endpoint in multiple iterations starting with a larger base offload followed by a few smaller incremental offloads. The test also verifies downloads by restoring the data from each iteration.

Since this test is designed to validate basic functionality, by default it is configured to transfer a relatively small amount of data.

*Performance test*

This test evaluates the performance of the offload feature.

The test performs a single upload session to the S3 endpoint and measures the write throughput. It also verifies downloads by restoring the data and measuring the read throughput.

Since the goal of this test is to measure throughput, by default it is configured to transfer a larger amount of data compared to the functional test.

*Scale test*

This test evaluates the performance and scalability of the offload feature by driving multiple concurrent offload operations.

The test performs multiple uploads sessions to the S3 endpoint concurrently and measures the average write throughput. The test can be repeated with different concurrency settings to evaluate how the performance scales as the number of sessions increases.

=== Configure the cloud endpoint

Before running the test suite, you must configure it to provide details regarding the S3 endpoint you want to test against.

To configure the endpoint details, modify the following file and edit the values:

----
/home/serveradmin/s3validator/tests/config/cloud_endpoint.json
----

Sample contents:

----
{
    "endpoint": "https://s3.example.com",
    "api_key": "xxxxxxxx",
    "api_secret": "yyyyyyyy",
    "bucket": "sample-bucket",
    "provider": "generic"
}
----

Fields in `cloud_endpoint.json`:

[cols="30%a,70%a", options="header"]
|====
|Field|Description
|`endpoint`|Specify the endpoint URL to be used for the tests. The URL must include the prefix `http://` or `https://`. For example: `https://s3.amazonaws.com`.
|`api_key`|Specify the Access Key for the endpoint.
|`api_secret`|Specify the Secret Key for the endpoint.
|`bucket`|Specify the name of the bucket that will be used for the tests.
|`provider`|Specify the provider type of the endpoint. Valid values:

* `cos`: IBM Cloud Object Storage
* `sp`: IBM Spectrum Protect
* `aws`: Amazon S3
* `azure`: Microsoft Azure Blob Storage
* `generic`: Any other S3-compatible endpoint

|====

=== Configure the test parameters



Before running the test suite,








<<<
== Configuration

The test scripts are divided into three categories as follows:

. Functional - These tests will run offload and restore using the registered s3 provider
. Performance - This test will tell us the throughput for a base offload and restore
. Scale - This test allows us to measure performance of the vSnap by varying the number of offloads it handles concurrently

=== Configuring S3 Provider

* To run tests, you need to first provide the details of the S3
provider. To do this, edit the following file:

    tests/config/cloud_endpoint.json

and provide the following information:

* endpoint ->  URL of the S3 provider.
* api_key -> Access key to be used to login.
* api_secret -> Password for the key provided above.
* bucket -> Bucket name.
* provider ->  You can use the default value.

=== Test Configuration

In directory tests update the pytest.ini file to set the offload size in MB's. Following are the fields you can configure, the default values are
mentioned for reference.

* Total time out (10800 seconds)

* Functional
** Base offload (10 MB)
** Incremental offload (5 MB)
** Number of increments (3)


* Performance
** Base offload size (1000 MB)

* Scale
** Base offload Size (10 MB)
** Number of offloads (10)
** Max vsnap streams (3)


<<<
== Test execution

Run the tests form the s3validator directory:

* Functional:

[source, bash]
----
$ ./runtests.sh functional
----

sample output:
image:functionaloutput.png[Functional test output]

* Performance:

[source, bash]
----
$ ./runtests.sh performance
----
sample output:
image:scaleoutput.png[Scale test output]

* Scale:

[source, bash]
----
$ ./runtests.sh scale

----
sample output:
image:performanceoutput.png[Performance test output]



The script will print information on the console as it runs each
test. Information about tests (such as APIs called and any errors) is
saved in the following two files:

* apiscalled.log
* test-results.xml

In case of any errors, please provide these files to IBM for
debugging.




